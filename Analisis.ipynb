{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datathon: COVID-19 USA\n",
    "## Importación de la base de datos.\n",
    "\n",
    "Para importar la base de datos, se va a utilizar la biblioteca requests para solicitar los datos directamente a la página. De esta forma estandarizamos la adiquisición de los datos para que todos los usuarios puedan obtener los mismos resultados. \n",
    "\n",
    "Otra opción es descargar los datos directamente en formato JSON o CSV desde la página web del proyecto, y posteriormente importarlos en python empleando los métodos apropiados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url = \"https://api.covidtracking.com/v1/states/daily.json\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    states_data = response.json()\n",
    "\n",
    "    df = pd.DataFrame(states_data)\n",
    "\n",
    "else:\n",
    "    print(f\"Error al solicitar los datos. Código {response.status_code}\")\n",
    "\n",
    "\n",
    "# Si queremos usar el archivo descargado utilizar pd.read_json o pd.read_csv en función del archivo descargado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de la base de datos.\n",
    "\n",
    "Una vez se han importado los datos, se debe decidir la forma en la que se va a comenzar la aproximación a la base. Dependiendo del trabajo de investigación que estemos realizando, o de lo que se nos haya solicitado en un entorno laboral, existen dos posibilidades:\n",
    "\n",
    "- Trabajar con la totalidad de la base de datos y \"pescar\" resultados.\n",
    "- Elegir una serie de variables de interés y extraer conclusiones.\n",
    "\n",
    "Un detalle importante, que por el tipo de trabajo ante el que nos encontramos no estamos teniendo en cuenta, es que toda investigación tiene que ir apoyada sobre una base teórica. Cualquier investigación requiere que tengamos unos conceptos previos que lleven de la mano la investigación. Por ejemplo, en este proyecto de casos de COVID en EEUU, podríamos plantear una posible investigación que explore la hipótesis de que los estados con mayor número de vacunados son los estados que más ven reducida la mortalidad por COVID. Pero para hacer este proceso, necesitaríamos haber realizado previamente una revisión teórica que avale el sentido de esta hipótesis. La extensión de la misma depende mucho del entorno en el que nos estemos desarrollando, y en el caso de entornos laborales empresariales no encontraremos esta limitación a menudo, pero es tarea nuestra como expertos el aportar los buenos haceres en el trabajo.\n",
    "\n",
    "Si bien es cierto que hoy en día con la expansión y mejora de los modelos de redes neuronales, machine learning, e IA en muchas ocasiones se plantean investigaciones en las que simplemente se buscan resultados \"a peso\". Es decir, se introducen todas las variables de las que disponemos y esperamos que se nos devuelva el mejor modelo posible. Hacer esto por supuesto es muy inoperante a nivel científico, ya que no disponer de una buena base teórica puede llevarnos a aceptar resultados totalmente incongruentes con la realidad, y a no tener en cuenta factores externos (variables de confusión, correlaciones espúreas, etc.) o posibles sesgos en la elaboración del modelo (¿son nuestros datos de calidad?). \n",
    "\n",
    "Otro punto importante a destacar es que, en investigación científica, trabajar con bases de datos preexistentes no siempre es lo más recomendado, ya que es una parte del proceso de investigación sobre el que perdemos el control, y no podemos determinar si se han llevado a cabo los controles suficientes y necesarios sobre la recogida de datos. Además, hay que tener en cuenta que no todo tipo de dato admite todo tipo de prueba estadística. Cuando se trabaja con modelos de machine learning nos encontraremos con casos en los que no se presta mucha atención a estas limitaciones, pero es importante considerar que tipo de dato disponemos en función de su recogida (transversal o longitudinal), en función de su tipología (binario, multinomial, continuo...) y en función del cumplimiento de supuestos en las variables relevantes.\n",
    "\n",
    "Hecho este preámbulo, las consideraciones para limpiar la base de datos en este caso serán:\n",
    "- Variables de interés (columnas que queremos quedarnos)\n",
    "- Adecuación del formato (revisar que los datos sean congruentes con la variable)\n",
    "- Análisis de datos perdidos (revisar el porcentaje de datos perdidos y considerar si merece la pena imputarlos)\n",
    "- Análisis de valores atípicos (empleando la generalización de datos 3 desviaciones típicas por encima de la media)\n",
    "\n",
    "El primer paso por tanto será decidir que variables serán de interés. Al tratarse de una base de datos amplia, y para facilitar el trabajo, podemos hacer una exploración de datos generales que sean relevantes y analizar si existen algunas diferencias. Ya que disponemos de medidas en distintas fechas de muertes, hospitalizaciones, casos recuperados, tests positivos y negativos, podemos realizar una exploración de la evolución temporal por estado de cada una de estas variables. Además, podemos plantear realizar algún tipo de análisis para ver si existieron diferencias significativas entre el número de hospitalizaciones y muertes entre los diferentes estados de EEUU.\n",
    "\n",
    "Para seleccionar correctamente los datos en este caso basta con mirar el nombre de las columnas de la base de datos, ya que son bastante descriptivos. No obstante, en muchos casos nos encontraremos con la necesidad de recurrir a la documentación del proyecto para determinar a que se refiere cada variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       date state  positive   negative  hospitalized  deathConfirmed\n",
      "0  20210307    AK   56886.0        NaN        1293.0             NaN\n",
      "1  20210307    AL  499819.0  1931711.0       45976.0          7963.0\n",
      "2  20210307    AR  324818.0  2480716.0       14926.0          4308.0\n",
      "3  20210307    AS       0.0     2140.0           NaN             NaN\n",
      "4  20210307    AZ  826454.0  3073010.0       57907.0         14403.0\n",
      "date                int64\n",
      "state              object\n",
      "positive          float64\n",
      "negative          float64\n",
      "hospitalized      float64\n",
      "deathConfirmed    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Seleccionamos las variables que nos interesan\n",
    "\n",
    "df_subset = df[[\"date\", \"state\", \"positive\", \"negative\", \"hospitalized\", \"deathConfirmed\"]]\n",
    "\n",
    "#Comprobamos la extracción de datos con head\n",
    "print(df_subset.head())\n",
    "\n",
    "#Comprobamos los tipos de dato con dtypes\n",
    "print(df_subset.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al comprobar los datos de cada columna encontramos un primer punto de mejora. Los datos de fecha están guardados como un entero (_int64_). Como vamos a querer explorar los datos y realizar presentaciones gráficas teniendo en cuenta las fechas, podemos modificar o generar una nueva columna con el formato de fecha apropiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       date state  positive   negative  hospitalized  deathConfirmed  \\\n",
      "0  20210307    AK   56886.0        NaN        1293.0             NaN   \n",
      "1  20210307    AL  499819.0  1931711.0       45976.0          7963.0   \n",
      "2  20210307    AR  324818.0  2480716.0       14926.0          4308.0   \n",
      "3  20210307    AS       0.0     2140.0           NaN             NaN   \n",
      "4  20210307    AZ  826454.0  3073010.0       57907.0         14403.0   \n",
      "\n",
      "    datetime  \n",
      "0 2021-03-07  \n",
      "1 2021-03-07  \n",
      "2 2021-03-07  \n",
      "3 2021-03-07  \n",
      "4 2021-03-07  \n"
     ]
    }
   ],
   "source": [
    "# Creamos la columna datetime aplicando una función lambda que modifique linea a linea el dato en int y lo paso a formato fecha\n",
    "df_subset.loc[:, 'datetime'] = df['date'].apply(lambda x: pd.to_datetime(str(x), format = '%Y%m%d'))\n",
    "\n",
    "print(df_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación podemos comprobar la cantidad de datos perdidos que encontramos en la base de datos que hemos generado. Es importante tener en cuenta al hacer este análisis de cual es el total de datos incluidos en la base, para poder tener en cuenta el porcentaje de perdidos que encontramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date                  0\n",
      "state                 0\n",
      "positive            188\n",
      "negative           7490\n",
      "hospitalized       8398\n",
      "deathConfirmed    11358\n",
      "datetime              0\n",
      "dtype: int64\n",
      "20780\n"
     ]
    }
   ],
   "source": [
    "print(df_subset.isna().sum()) # Datos perdidos por columna\n",
    "print(len(df_subset.index)) # Datos totales en la base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en la celda previa, existen una gran cantidad de datos perdidos entre nuestros datos, particularmente en la variable deathConfirmed donde más del 50% de los datos son perdidos. Ante esta situación antes de tomar decisiones sobre imputación, hay que evaluar que tipología de dato perdido presentan.\n",
    "\n",
    "Los datos perdidos pueden categorizarse en tres grupos:\n",
    "- MCAR (_Missing completely at random_): Será nuestra forma favorita de dato perdido, en este caso la probabilidad de que el dato se pierda es la misma para todos los casos de la base de datos, esto implica que las causas por las que se ha perdido el dato es ajena a la base de datos. Ejemplo de este tipo de datos perdidos es cuando se escoge una muestra aleatoria de una población, donde cada individuo tiene la misma probabilidad de ser incluido en la muestra, por lo que perder datos puede obedecer a mala suerte. A pesar de que es la forma ideal de perder datos, es poco realista.\n",
    "- MAR (_Missing at random_): En este caso la probabilidad de que el dato se haya perdido es la misma pero solo en grupos definidos por los datos observables. Por ejemplo, *hipotéticamente*, en nuestro caso podría darse que en el estado de Kansas hay más datos perdidos que en el resto de estados, ya que tuvieron dificultades a la hora de registrar los datos por falta de personal. Si sabemos que este estado es el que genera más valores perdidos y podemos asumir MCAR *DENTRO* de la categoría de Kansas, los datos se consideran MAR. Es la opción más realista de dato perdido y sobre la que generalmente se trabaja, es importante recordar que los métodos de imputación asumen MCAR pero al no ser realista trabajamos con ellos en datos MAR asumiendo que puede haber sesgos.\n",
    "- MNAR (_Missing not at random_): El peor caso de datos perdido, ya que los métodos de imputación no son recomendables bajo este tipo de dato ya que producen sesgos importantes. Este tipo de dato perdido refiere a que desconocemos de las causas de la probabilidad de que un dato sea perdido. Un ejemplo clásico de MNAR es en estudios de opinion pública, la gente que tiene opiniones menos fuertes sobre un tema tienden a responder menos a estas encuestas, por lo que se pierde su dato y no tenemos forma de \"recuperarlo\". La única solución a MNAR es realizar más estudios que analicen las causas de la pérdida o plantear escensarios _what if_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
